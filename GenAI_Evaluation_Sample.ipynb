{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fcf8d-30c9-4543-af41-9f9fe086d1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pointwise 검사를 위한 데이터셋 준비\n",
    "#prompt(Source), reference, response\n",
    "data = [\n",
    "    [\"This account is not protected with a two-factor authentication.\", \n",
    "     \"2차 인증이 되지 않은 계정입니다.\",\n",
    "     \"이중 인증으로 보호되지 않은 계정입니다.\"],\n",
    "    [\"Even the longest journey begins with a single step.\",\n",
    "    \"천 리 길도 한 걸음부터.\",\n",
    "    \"가장 먼 여정도 한 걸음에서 시작됩니다.\"],\n",
    "    [\"Even the longest journey begins with a single step.\",\n",
    "    \"Лиха беда начало.\",\n",
    "    \"Даже самая длинная дорога продолжается с одного шага.\"]\n",
    "]\n",
    "pointwise_df = pd.DataFrame(data, columns=['prompt', 'reference', 'response'])\n",
    "pointwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5ce08-2681-498a-ace3-875ff44710d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PairwiseMetric\n",
    "pointwise_ground_truth_metric_prompt = \"\"\"\n",
    "# Instruction\n",
    "당신은 전문적인 번역 품질 평가자 입니다. 당신은 AI 모델이 번역한 내용의 품질을 평가해야 합니다.\n",
    "우리는 당신에게 'Source, AI translation, Ground Truth'를 제공할 것입니다.\n",
    "업무를 수행하기 위해 'Source, AI translation, Ground Truth'을 신중하게 읽고, 아래의 'Evaluation' 섹션에 정의된 'Evaluation criteria'에 근거하여 평가해야 합니다.\n",
    "당신은 'Rating rubric'과 'Evaluation steps'에 기반하여 평가해야 합니다. 평가의 이유에 대해 단계별로 '한글'로 설명하고 'Rating rubric'의 순위만 선택해야 합니다.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "번역이 'Ground Truth'와 동일하게 번역되었는지, 그렇지 않으면 의미적으로 유사하고 간결, 명확, 캐주얼하게 작성되었는지 전반적으로 확인합니다.\n",
    "\n",
    "## Evaluation criteria\n",
    "예시 일치성: 예시로 제공되는 'Ground Truth'와 얼마나 일치하는지 확인합니다.\n",
    "지시를 잘 따르는지: 'AI translation'이 'Evaluation Definition'을 잘 따라서 수행됐는지 확인합니다.\n",
    "\n",
    "## Rating rubric\n",
    "5: (매우 좋음). 'Ground Truth'와 글자수 등 표현이 정확하게 일치함\n",
    "4: (좋음). 'Ground Truth'와 어순 외 차이 없음\n",
    "3: (괜찮음). 속담이나 관용적 표현을 사용하지 못했지만 이해하는데 큰 문제 없음\n",
    "2: (나쁨). 이해하기가 어려움\n",
    "1: (매우 나쁨). 번역결과가 'Ground Truth'와 매우 다름\n",
    "\n",
    "## Evaluation steps\n",
    "STEP 1: 'Ground Truth'을 참고하여 'AI translation'이 번역한 내용을 비교합니다.\n",
    "STEP 2: 평가 규칙에 따라 점수를 부여합니다.\n",
    "\n",
    "# Source, AI translation, Ground Truth\n",
    "### Source\n",
    "{prompt}\n",
    "\n",
    "### Ground Truth\n",
    "{reference}\n",
    "\n",
    "### AI translation\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "pointwise_ground_truth_text_quality = PointwiseMetric(\n",
    "    metric=\"pointwise_ground_truth_quality\",\n",
    "    metric_prompt_template=pointwise_ground_truth_metric_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43285b27-eb15-44d8-8794-4802e4bb2c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_task = EvalTask(dataset=pointwise_df, \n",
    "                    metrics=[pointwise_ground_truth_text_quality, \"bleu\", \"rouge\"],\n",
    "                    experiment=\"pointwise-eval\")\n",
    "result_pointwise = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674a2e9-00ae-4b97-b06e-b67323bdbacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_pointwise.summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840104e0-b684-48d7-ae2d-32b87f9ff5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "result_pointwise.metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72203b0e-be36-4478-a5a5-735bf004f05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pairwise 검사를 위한 데이터셋 준비\n",
    "#prompt(Source), reference, response (Response B), baseline_model_response (Response A)\n",
    "data = [\n",
    "    [\"This account is not protected with a two-factor authentication.\", \n",
    "     \"2차 인증이 되지 않은 계정입니다.\",\n",
    "     \"이중 인증으로 보호되지 않은 계정입니다.\",\n",
    "     \"이 계정은 이중 인증으로 안전하게 보호되지 않습니다.\"],\n",
    "    [\"Even the longest journey begins with a single step.\",\n",
    "    \"천 리 길도 한 걸음부터.\",\n",
    "    \"가장 먼 여정도 한 걸음에서 시작됩니다.\",\n",
    "    \"가장 긴 여행조차도 한 걸음부터 시작됩니다.\"],\n",
    "    [\"Even the longest journey begins with a single step.\",\n",
    "    \"Лиха беда начало.\",\n",
    "    \"Даже самая длинная дорога продолжается с одного шага.\",\n",
    "    \"Даже самый долгий путь начинается с первого шага.\"]\n",
    "]\n",
    "pairwise_df = pd.DataFrame(data, columns=['prompt', 'reference', 'response', 'baseline_model_response'])\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dbe29-d5eb-4415-9528-3f07fc8d378c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairwise_model_compare_metric_prompt = \"\"\"\n",
    "# Instruction\n",
    "당신은 전문적인 번역 품질 평가자 입니다. 당신은 2개의 AI 모델이 번역한 내용의 품질을 평가해야 합니다.\n",
    "우리는 당신에게 'Source, Response A, Response B, Ground Truth'를 제공할 것입니다.\n",
    "업무를 수행하기 위해 'Source, Response A, Response B, Ground Truth'을 신중하게 읽고, 아래의 '평가' 섹션에 정의된 '평가항목'에 근거하여 평가해야 합니다.\n",
    "당신은 'Evaluation rule'과 'Evaluation steps'에 기반하여 평가해야 합니다. 평가의 이유에 대해 단계별로 '한글'로 설명하고 'Evaluation rule'의 순위만 선택해야 합니다.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "번역이 'Ground Truth'와 동일하게 번역되었는지, 그렇지 않으면 의미적으로 유사하고 간결, 명확, 캐주얼하게 작성되었는지 전반적으로 확인합니다.\n",
    "\n",
    "## Evaluation criteria\n",
    "예시 일치성: 예시로 제공되는 'Ground Truth'와 얼마나 일치하는지 확인합니다.\n",
    "지시를 잘 따르는지: 'AI translation'이 'Evaluation Definition'을 잘 따라서 수행됐는지 확인합니다.\n",
    "\n",
    "## Rating rubric\n",
    "STEP 1: Analyze Response A based on all the Criteria.\n",
    "STEP 2: Analyze Response B based on all the Criteria.\n",
    "STEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\n",
    "STEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\n",
    "STEP 5: Output your assessment reasoning in the explanation field.\n",
    "\n",
    "# Source, Response A, Response B, Ground Truth\n",
    "### Source\n",
    "{prompt}\n",
    "\n",
    "### Ground Truth\n",
    "{reference}\n",
    "\n",
    "### Response A\n",
    "{baseline_model_response}\n",
    "\n",
    "### Response B\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "pairwise_ground_truth_text_quality = PairwiseMetric(\n",
    "    metric=\"pairwise_ground_truth_quality\",\n",
    "    metric_prompt_template=pairwise_model_compare_metric_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466154ef-2d0b-482a-b709-15f8547ec207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_task = EvalTask(dataset=pairwise_df, \n",
    "                    metrics=[pairwise_ground_truth_text_quality],\n",
    "                    experiment=\"pairwise-eval\")\n",
    "result_pairwise = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d224649-87d2-4e28-94e8-43676c164c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_pairwise.summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111f528-8cb6-4d9d-a0f4-e7f94475f8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "result_pairwise.metrics_table.rename(columns={\"baseline_model_response\": \"Response A\", \"response\": \"Response B\"}).replace(['BASELINE', 'CANDIDATE'], ['A', 'B'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
